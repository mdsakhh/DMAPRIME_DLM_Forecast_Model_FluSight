{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "32ae2753-01be-4695-be26-3e8a1f7132c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "====================================================================================================\n",
      "REAL-TIME DLM RECURSIVE FORECAST — SUBMISSION FILE ONLY\n",
      "====================================================================================================\n",
      "✓ Loaded params: C:\\Users\\mdsakhh\\Box\\BoxPHI-PHMR Projects\\Sakhawat\\FluSight_Forecast\\model_eval\\optimal_parameters\\optimal_params_dlm_recursive_hosp_with_pos_inp_prisma_musc_log.json\n",
      "Combined EHR: 487 weeks | 2016-10-08 to 2026-01-31\n",
      "\n",
      "====================================================================================================\n",
      "LOCATION: South Carolina (45)\n",
      "====================================================================================================\n",
      "Data: 209 weeks | 2022-02-05 to 2026-01-31\n",
      "Config: y_lags=[1, 2, 3, 4, 5, 6, 7], pos_lags=[5], inp_lags=[5, 6, 7, 8, 9]\n",
      "\n",
      "Origin: 2026-01-31 | Reference date: 2026-02-07\n",
      "  h=0 @ 2026-02-07 | median=185 | 50%CI=(140,245) | 95%CI=(82,417)\n",
      "  h=1 @ 2026-02-14 | median=113 | 50%CI=(85,149) | 95%CI=(50,255)\n",
      "  h=2 @ 2026-02-21 | median=66 | 50%CI=(50,88) | 95%CI=(29,150)\n",
      "  h=3 @ 2026-02-28 | median=47 | 50%CI=(35,63) | 95%CI=(20,107)\n",
      "✓ Saved plot: C:\\Users\\mdsakhh\\Box\\BoxPHI-PHMR Projects\\Sakhawat\\FluSight_Forecast\\final_submission\\dlm_recursive_hosp_with_pos_inp_prisma_musc_log\\realtime_latest_fit_all_data_SC_US\\plots\\45_hosp_dlm_realtime_ref_2026-02-07.png\n",
      "\n",
      "====================================================================================================\n",
      "LOCATION: US (US)\n",
      "====================================================================================================\n",
      "Data: 209 weeks | 2022-02-05 to 2026-01-31\n",
      "NOTE: Using SC EHR series for US.\n",
      "Config: y_lags=[1, 2, 3], pos_lags=[6, 8, 10], inp_lags=[5, 6, 7, 8, 9, 10]\n",
      "\n",
      "Origin: 2026-01-31 | Reference date: 2026-02-07\n",
      "  h=0 @ 2026-02-07 | median=12994 | 50%CI=(11744,14378) | 95%CI=(9683,17438)\n",
      "  h=1 @ 2026-02-14 | median=10990 | 50%CI=(9932,12161) | 95%CI=(8190,14748)\n",
      "  h=2 @ 2026-02-21 | median=9015 | 50%CI=(8147,9975) | 95%CI=(6718,12098)\n",
      "  h=3 @ 2026-02-28 | median=7015 | 50%CI=(6340,7762) | 95%CI=(5227,9414)\n",
      "✓ Saved plot: C:\\Users\\mdsakhh\\Box\\BoxPHI-PHMR Projects\\Sakhawat\\FluSight_Forecast\\final_submission\\dlm_recursive_hosp_with_pos_inp_prisma_musc_log\\realtime_latest_fit_all_data_SC_US\\plots\\US_hosp_dlm_realtime_ref_2026-02-07.png\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "SAVING SUBMISSION FILE\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Saving submission CSV (SC+US)...\n",
      "  Target: C:\\Users\\mdsakhh\\Box\\BoxPHI-PHMR Projects\\Sakhawat\\FluSight_Forecast\\final_submission\\dlm_recursive_hosp_with_pos_inp_prisma_musc_log\\realtime_latest_fit_all_data_SC_US\\FluSight_submission_DLM_EHR_SC_US_ref_2026-02-07.csv\n",
      "  ✓ File saved and verified: C:\\Users\\mdsakhh\\Box\\BoxPHI-PHMR Projects\\Sakhawat\\FluSight_Forecast\\final_submission\\dlm_recursive_hosp_with_pos_inp_prisma_musc_log\\realtime_latest_fit_all_data_SC_US\\FluSight_submission_DLM_EHR_SC_US_ref_2026-02-07.csv\n",
      "    Size: 10,742 bytes | Rows: 184\n",
      "\n",
      "====================================================================================================\n",
      "✓ SUBMISSION READY (SC+US)\n",
      "Reference date: 2026-02-07\n",
      "Rows: 184 (2 locations × 4 horizons × 23 quantiles)\n",
      "Saved to: C:\\Users\\mdsakhh\\Box\\BoxPHI-PHMR Projects\\Sakhawat\\FluSight_Forecast\\final_submission\\dlm_recursive_hosp_with_pos_inp_prisma_musc_log\\realtime_latest_fit_all_data_SC_US\\FluSight_submission_DLM_EHR_SC_US_ref_2026-02-07.csv\n",
      "====================================================================================================\n",
      "\n",
      "DONE\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# FluSight — REAL-TIME FINAL DLM RECURSIVE FORECAST (+EHR POS + EHR INPATIENT LAGS)\n",
    "# SUBMISSION FILE ONLY — SC + US\n",
    "#\n",
    "# What this script does:\n",
    "#   ✅ Loads optimal DLM(+EHR pos+inp) params JSON from your Box folder (SC + US keys)\n",
    "#   ✅ Loads Prisma+MUSC weekly influenza EHR series (SC)\n",
    "#   ✅ Uses latest FluSight target-hospital-admissions from GitHub (SC + US)\n",
    "#   ✅ REAL-TIME forecast:\n",
    "#        - ORIGIN = last observed target_end_date in each location series\n",
    "#        - reference_date = ORIGIN + 7 days (h=0 target_end_date)\n",
    "#        - Fit model on ALL available data (1-step pairs) for each location\n",
    "#        - Forecast h=0..3 (1..4 weeks ahead) recursively\n",
    "#        - Quantiles via Normal approx using se_mean, inverse-transform, clip0, ROUND int\n",
    "#   ✅ Saves:\n",
    "#        - One combined submission CSV for SC + US\n",
    "#        - One plot per location (observed full series + 50%/95% forecast bands)\n",
    "#\n",
    "# Requirements:\n",
    "#   pip install pandas numpy matplotlib statsmodels scipy\n",
    "# =============================================================================\n",
    "\n",
    "import os\n",
    "import glob\n",
    "import json\n",
    "import warnings\n",
    "import time\n",
    "import shutil\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from scipy.stats import norm\n",
    "from statsmodels.tsa.statespace.structural import UnobservedComponents\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# =============================================================================\n",
    "# CONFIG\n",
    "# =============================================================================\n",
    "base_dir = r\"C:\\Users\\mdsakhh\\Box\\BoxPHI-PHMR Projects\\Sakhawat\\FluSight_Forecast\"\n",
    "\n",
    "FILE_HOSP = (\n",
    "    \"https://raw.githubusercontent.com/cdcepi/FluSight-forecast-hub/\"\n",
    "    \"main/target-data/target-hospital-admissions.csv\"\n",
    ")\n",
    "\n",
    "# --- EHR Data paths ---\n",
    "PRISMA_DIR = r\"C:\\Users\\mdsakhh\\Box\\BoxPHI-PHMR Projects\\Data\\Prisma Health\\Infectious Disease EHR\\Weekly Data\\Latest Weekly Data\"\n",
    "PRISMA_BASENAME = \"Prisma_Health_Weekly_Influenza_State_dx_Incident\"\n",
    "MUSC_DIR = r\"C:\\Users\\mdsakhh\\Box\\BoxPHI-PHMR Projects\\Data\\MUSC\\Infectious Disease EHR\\Weekly Data\\Latest Weekly Data\"\n",
    "MUSC_BASENAME = \"MUSC_Weekly_Influenza_State_dx_Incident\"\n",
    "\n",
    "APPLY_SC_EHR_TO_US = True\n",
    "\n",
    "LOCATIONS = [\n",
    "    dict(code=\"45\", name=\"South Carolina\"),\n",
    "    dict(code=\"US\", name=\"US\"),\n",
    "]\n",
    "\n",
    "OUTCOME_MEASURE = \"wk inc flu hosp\"\n",
    "TARGET_KEY = \"hosp\"\n",
    "HORIZONS = [0, 1, 2, 3]\n",
    "\n",
    "USE_LOG_TRANSFORM = True\n",
    "MIN_TRAIN_ROWS = 60\n",
    "\n",
    "SUBMISSION_QUANTILES = [\n",
    "    0.01, 0.025, 0.05,\n",
    "    0.10, 0.15, 0.20, 0.25, 0.30, 0.35, 0.40, 0.45, 0.50,\n",
    "    0.55, 0.60, 0.65, 0.70, 0.75, 0.80, 0.85, 0.90,\n",
    "    0.95, 0.975, 0.99\n",
    "]\n",
    "\n",
    "ROUND_NDIGITS = 0\n",
    "\n",
    "suffix = \"log\" if USE_LOG_TRANSFORM else \"raw\"\n",
    "\n",
    "DLM_PARAMS_JSON = os.path.join(\n",
    "    base_dir, \"model_eval\", \"optimal_parameters\",\n",
    "    f\"optimal_params_dlm_recursive_hosp_with_pos_inp_prisma_musc_{suffix}.json\"\n",
    ")\n",
    "\n",
    "# Output\n",
    "out_root = os.path.join(\n",
    "    base_dir,\n",
    "    \"final_submission\",\n",
    "    f\"dlm_recursive_hosp_with_pos_inp_prisma_musc_{suffix}\",\n",
    "    \"realtime_latest_fit_all_data_SC_US\"\n",
    ")\n",
    "out_plot_dir = os.path.join(out_root, \"plots\")\n",
    "\n",
    "for d in [out_root, out_plot_dir]:\n",
    "    try:\n",
    "        os.makedirs(d, exist_ok=True)\n",
    "    except Exception as e:\n",
    "        print(f\"WARNING: Could not create directory {d}: {e}\")\n",
    "\n",
    "# =============================================================================\n",
    "# ROBUST FILE SAVING\n",
    "# =============================================================================\n",
    "def safe_save_csv(df, filepath, max_retries=3, retry_delay=2):\n",
    "    \"\"\"Safely save DataFrame to CSV with verification and retries.\"\"\"\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            os.makedirs(os.path.dirname(filepath), exist_ok=True)\n",
    "            temp_path = filepath + f\".tmp_{int(time.time())}\"\n",
    "            \n",
    "            df.to_csv(temp_path, index=False, encoding='utf-8', lineterminator='\\n')\n",
    "            time.sleep(0.5)\n",
    "            \n",
    "            if os.path.exists(temp_path):\n",
    "                file_size = os.path.getsize(temp_path)\n",
    "                if file_size == 0:\n",
    "                    print(f\"  WARNING: Temp file is empty (attempt {attempt+1})\")\n",
    "                    if os.path.exists(temp_path):\n",
    "                        os.remove(temp_path)\n",
    "                    continue\n",
    "                \n",
    "                try:\n",
    "                    test_df = pd.read_csv(temp_path)\n",
    "                    if len(test_df) != len(df):\n",
    "                        print(f\"  WARNING: Row count mismatch (attempt {attempt+1})\")\n",
    "                        if os.path.exists(temp_path):\n",
    "                            os.remove(temp_path)\n",
    "                        continue\n",
    "                except Exception as read_err:\n",
    "                    print(f\"  WARNING: Could not verify temp file (attempt {attempt+1}): {read_err}\")\n",
    "                    if os.path.exists(temp_path):\n",
    "                        os.remove(temp_path)\n",
    "                    continue\n",
    "                \n",
    "                if os.path.exists(filepath):\n",
    "                    try:\n",
    "                        os.remove(filepath)\n",
    "                        time.sleep(0.3)\n",
    "                    except Exception:\n",
    "                        pass\n",
    "                \n",
    "                shutil.move(temp_path, filepath)\n",
    "                time.sleep(0.5)\n",
    "                \n",
    "                if os.path.exists(filepath):\n",
    "                    final_size = os.path.getsize(filepath)\n",
    "                    if final_size > 0:\n",
    "                        try:\n",
    "                            final_df = pd.read_csv(filepath)\n",
    "                            if len(final_df) == len(df):\n",
    "                                print(f\"  ✓ File saved and verified: {filepath}\")\n",
    "                                print(f\"    Size: {final_size:,} bytes | Rows: {len(df)}\")\n",
    "                                return True\n",
    "                        except Exception:\n",
    "                            pass\n",
    "                        \n",
    "            if os.path.exists(temp_path):\n",
    "                try:\n",
    "                    os.remove(temp_path)\n",
    "                except:\n",
    "                    pass\n",
    "                    \n",
    "        except PermissionError:\n",
    "            print(f\"  Attempt {attempt+1}/{max_retries}: Permission denied - file may be locked by Box sync\")\n",
    "            time.sleep(retry_delay)\n",
    "        except Exception as e:\n",
    "            print(f\"  Attempt {attempt+1}/{max_retries}: Error saving file: {e}\")\n",
    "            time.sleep(retry_delay)\n",
    "    \n",
    "    return False\n",
    "\n",
    "\n",
    "def save_with_fallback(df, primary_path, description=\"file\"):\n",
    "    \"\"\"Try primary path, fall back to Desktop if fails.\"\"\"\n",
    "    print(f\"\\nSaving {description}...\")\n",
    "    print(f\"  Target: {primary_path}\")\n",
    "    \n",
    "    if safe_save_csv(df, primary_path):\n",
    "        return primary_path\n",
    "    \n",
    "    print(f\"  Primary save failed. Trying fallback location...\")\n",
    "    fallback_dir = os.path.join(os.path.expanduser(\"~\"), \"Desktop\", \"FluSight_submissions_fallback\")\n",
    "    os.makedirs(fallback_dir, exist_ok=True)\n",
    "    fallback_path = os.path.join(fallback_dir, os.path.basename(primary_path))\n",
    "    \n",
    "    if safe_save_csv(df, fallback_path):\n",
    "        print(f\"  ✓ Saved to fallback: {fallback_path}\")\n",
    "        return fallback_path\n",
    "    \n",
    "    print(f\"  Fallback also failed. Saving to current directory...\")\n",
    "    last_resort = os.path.basename(primary_path)\n",
    "    try:\n",
    "        df.to_csv(last_resort, index=False, encoding='utf-8')\n",
    "        print(f\"  ✓ Saved to current directory: {os.path.abspath(last_resort)}\")\n",
    "        return os.path.abspath(last_resort)\n",
    "    except Exception as e:\n",
    "        print(f\"  ERROR: All save attempts failed: {e}\")\n",
    "        return None\n",
    "\n",
    "# =============================================================================\n",
    "# UTIL\n",
    "# =============================================================================\n",
    "def _latest_file_by_mtime(file_list):\n",
    "    return max(file_list, key=lambda p: os.path.getmtime(p)) if file_list else None\n",
    "\n",
    "def pick_col(df, candidates):\n",
    "    for c in candidates:\n",
    "        if c in df.columns:\n",
    "            return c\n",
    "    return None\n",
    "\n",
    "def fmt_mdY(dt_like):\n",
    "    dt = pd.to_datetime(dt_like)\n",
    "    if isinstance(dt, pd.Series):\n",
    "        s = dt.dt.strftime(\"%m/%d/%Y\")\n",
    "        s = s.str.replace(r\"^0\", \"\", regex=True)\n",
    "        s = s.str.replace(r\"/0\", \"/\", regex=True)\n",
    "        return s\n",
    "    s = dt.strftime(\"%m/%d/%Y\")\n",
    "    mm, dd, yy = s.split(\"/\")\n",
    "    return f\"{int(mm)}/{int(dd)}/{yy}\"\n",
    "\n",
    "def log1p_safe(x):\n",
    "    return np.log1p(np.maximum(0.0, x))\n",
    "\n",
    "def inv_log1p(x):\n",
    "    return np.expm1(x)\n",
    "\n",
    "def to_scalar(x):\n",
    "    return float(np.asarray(x).ravel()[0])\n",
    "\n",
    "def round_forecast_value(x, ndigits=0):\n",
    "    x = float(max(0.0, x))\n",
    "    if ndigits == 0:\n",
    "        return int(round(x))\n",
    "    return float(round(x, ndigits))\n",
    "\n",
    "def _q_str(q):\n",
    "    s = f\"{q:.3f}\".rstrip(\"0\").rstrip(\".\")\n",
    "    return s\n",
    "\n",
    "# =============================================================================\n",
    "# HOLIDAY FLAGS\n",
    "# =============================================================================\n",
    "def thanksgiving_flag(d):\n",
    "    d = pd.Timestamp(d)\n",
    "    if d.month != 11:\n",
    "        return 0\n",
    "    first = pd.Timestamp(year=d.year, month=11, day=1)\n",
    "    offset = (3 - first.dayofweek) % 7\n",
    "    first_thu = first + pd.Timedelta(days=offset)\n",
    "    fourth_thu = first_thu + pd.Timedelta(days=21)\n",
    "    return int(abs((d - fourth_thu).days) <= 3)\n",
    "\n",
    "def christmas_flag(d):\n",
    "    d = pd.Timestamp(d)\n",
    "    return int(d.month == 12 and d.day >= 20)\n",
    "\n",
    "def newyear_flag(d):\n",
    "    d = pd.Timestamp(d)\n",
    "    return int(d.month == 1 and d.day <= 7)\n",
    "\n",
    "# =============================================================================\n",
    "# LOAD OPTIMAL PARAMS\n",
    "# =============================================================================\n",
    "def load_optimal_params():\n",
    "    if os.path.exists(DLM_PARAMS_JSON):\n",
    "        path = DLM_PARAMS_JSON\n",
    "    else:\n",
    "        candidates = glob.glob(\n",
    "            os.path.join(base_dir, \"model_eval\", \"optimal_parameters\",\n",
    "                         f\"optimal_params_dlm_recursive_hosp_with_pos_inp_prisma_musc_{suffix}*.json\")\n",
    "        )\n",
    "        path = _latest_file_by_mtime(candidates)\n",
    "        if path is None:\n",
    "            raise FileNotFoundError(f\"Could not find params JSON: {DLM_PARAMS_JSON}\")\n",
    "    with open(path, \"r\") as f:\n",
    "        obj = json.load(f)\n",
    "    print(f\"✓ Loaded params: {path}\")\n",
    "    return obj, path\n",
    "\n",
    "# =============================================================================\n",
    "# LOAD TARGET SERIES\n",
    "# =============================================================================\n",
    "def load_target_series_hosp(url, location_code, outcome_measure):\n",
    "    df = pd.read_csv(url)\n",
    "    df[\"date\"] = pd.to_datetime(df.get(\"target_end_date\", df.get(\"date\")))\n",
    "    df[\"location\"] = df[\"location\"].astype(str)\n",
    "    sub = df[df[\"location\"] == str(location_code)].copy()\n",
    "    if \"outcome_measure\" in sub.columns:\n",
    "        sub = sub[sub[\"outcome_measure\"] == outcome_measure].copy()\n",
    "    sub[\"y_original\"] = pd.to_numeric(sub[\"value\"], errors=\"coerce\")\n",
    "    sub = sub[[\"date\", \"y_original\"]].dropna().sort_values(\"date\").drop_duplicates(\"date\").reset_index(drop=True)\n",
    "    sub[\"y\"] = log1p_safe(sub[\"y_original\"].values) if USE_LOG_TRANSFORM else sub[\"y_original\"].astype(float)\n",
    "    return sub\n",
    "\n",
    "# =============================================================================\n",
    "# LOAD EHR (Prisma + MUSC)\n",
    "# =============================================================================\n",
    "def load_ehr_covariates_sc():\n",
    "    prisma_files = glob.glob(os.path.join(PRISMA_DIR, f\"{PRISMA_BASENAME}*.csv\"))\n",
    "    musc_files = glob.glob(os.path.join(MUSC_DIR, f\"{MUSC_BASENAME}*.csv\"))\n",
    "\n",
    "    def _load_one(path, tag):\n",
    "        df = pd.read_csv(path)\n",
    "        if \"State\" in df.columns:\n",
    "            df = df[df[\"State\"] == \"SC\"].copy()\n",
    "        wk_col = pick_col(df, [\"Week\", \"week\", \"Week_End\", \"week_end\", \"WeekEnd\", \"date\", \"Date\"])\n",
    "        if wk_col is None:\n",
    "            raise ValueError(f\"{tag}: week/date column not found\")\n",
    "        df[wk_col] = pd.to_datetime(df[wk_col])\n",
    "        pos_col = pick_col(df, [\"Weekly_Positive_Tests\", \"Weekly_Positive\", \"Positive_Tests\", \"Weekly_Positive_Tests_All\"])\n",
    "        inp_col = pick_col(df, [\"Weekly_Inpatient_Hospitalizations\", \"Weekly_Inpatient_Hosp\", \"Inpatient_Hospitalizations\"])\n",
    "        if pos_col is None:\n",
    "            df[\"__pos__\"] = 0.0\n",
    "            pos_col = \"__pos__\"\n",
    "        if inp_col is None:\n",
    "            df[\"__inp__\"] = 0.0\n",
    "            inp_col = \"__inp__\"\n",
    "        out = df[[wk_col, pos_col, inp_col]].copy()\n",
    "        out.columns = [\"date\", f\"{tag}_pos\", f\"{tag}_inp\"]\n",
    "        out[f\"{tag}_pos\"] = pd.to_numeric(out[f\"{tag}_pos\"], errors=\"coerce\").fillna(0.0)\n",
    "        out[f\"{tag}_inp\"] = pd.to_numeric(out[f\"{tag}_inp\"], errors=\"coerce\").fillna(0.0)\n",
    "        return out\n",
    "\n",
    "    pf, mf = _latest_file_by_mtime(prisma_files), _latest_file_by_mtime(musc_files)\n",
    "    prisma_df = _load_one(pf, \"prisma\") if pf else None\n",
    "    musc_df = _load_one(mf, \"musc\") if mf else None\n",
    "\n",
    "    if prisma_df is None and musc_df is None:\n",
    "        print(\"WARNING: No EHR files found.\")\n",
    "        return None\n",
    "\n",
    "    if prisma_df is not None and musc_df is not None:\n",
    "        ehr = pd.merge(prisma_df, musc_df, on=\"date\", how=\"outer\")\n",
    "    elif prisma_df is not None:\n",
    "        ehr = prisma_df.copy()\n",
    "        ehr[\"musc_pos\"], ehr[\"musc_inp\"] = 0.0, 0.0\n",
    "    else:\n",
    "        ehr = musc_df.copy()\n",
    "        ehr[\"prisma_pos\"], ehr[\"prisma_inp\"] = 0.0, 0.0\n",
    "\n",
    "    ehr = ehr.fillna(0.0).sort_values(\"date\").reset_index(drop=True)\n",
    "    ehr[\"pos\"] = ehr[\"prisma_pos\"] + ehr[\"musc_pos\"]\n",
    "    ehr[\"inp\"] = ehr[\"prisma_inp\"] + ehr[\"musc_inp\"]\n",
    "    if USE_LOG_TRANSFORM:\n",
    "        ehr[\"pos\"] = log1p_safe(ehr[\"pos\"].values)\n",
    "        ehr[\"inp\"] = log1p_safe(ehr[\"inp\"].values)\n",
    "    print(f\"Combined EHR: {len(ehr)} weeks | {ehr['date'].min().date()} to {ehr['date'].max().date()}\")\n",
    "    return ehr[[\"date\", \"pos\", \"inp\"]].copy()\n",
    "\n",
    "# =============================================================================\n",
    "# BUILD 1-STEP DATASET\n",
    "# =============================================================================\n",
    "def build_one_step_dataset_with_ehr(series_df, ehr_df, y_lags, pos_lags, inp_lags):\n",
    "    df = series_df.copy().sort_values(\"date\").reset_index(drop=True)\n",
    "    if ehr_df is not None:\n",
    "        df = pd.merge(df, ehr_df, on=\"date\", how=\"left\")\n",
    "        df[\"pos\"] = df[\"pos\"].fillna(0.0)\n",
    "        df[\"inp\"] = df[\"inp\"].fillna(0.0)\n",
    "    else:\n",
    "        df[\"pos\"], df[\"inp\"] = 0.0, 0.0\n",
    "\n",
    "    rows = []\n",
    "    for i in range(len(df) - 1):\n",
    "        origin_date, target_date = df.loc[i, \"date\"], df.loc[i + 1, \"date\"]\n",
    "        y_t, y_o = float(df.loc[i + 1, \"y\"]), float(df.loc[i + 1, \"y_original\"])\n",
    "        feats = {}\n",
    "        for L in y_lags:\n",
    "            j = i - L + 1\n",
    "            feats[f\"y_lag{L}\"] = float(df.loc[j, \"y\"]) if j >= 0 else float(df.loc[0, \"y\"])\n",
    "        for L in pos_lags:\n",
    "            j = i - L + 1\n",
    "            feats[f\"pos_lag{L}\"] = float(df.loc[j, \"pos\"]) if j >= 0 else float(df.loc[0, \"pos\"])\n",
    "        for L in inp_lags:\n",
    "            j = i - L + 1\n",
    "            feats[f\"inp_lag{L}\"] = float(df.loc[j, \"inp\"]) if j >= 0 else float(df.loc[0, \"inp\"])\n",
    "        feats[\"is_thanksgiving\"] = thanksgiving_flag(target_date)\n",
    "        feats[\"is_christmas\"] = christmas_flag(target_date)\n",
    "        feats[\"is_newyear\"] = newyear_flag(target_date)\n",
    "        row = {\"origin_date\": origin_date, \"target_end_date\": target_date, \"y\": y_t, \"y_original\": y_o}\n",
    "        row.update(feats)\n",
    "        rows.append(row)\n",
    "\n",
    "    ds = pd.DataFrame(rows)\n",
    "    exog_cols = sorted(\n",
    "        [c for c in ds.columns if c.startswith((\"y_lag\", \"pos_lag\", \"inp_lag\", \"is_\"))],\n",
    "        key=lambda x: (0 if x.startswith(\"y_lag\") else 1 if x.startswith(\"pos_lag\") else 2 if x.startswith(\"inp_lag\") else 3, x)\n",
    "    )\n",
    "    return ds, exog_cols, df\n",
    "\n",
    "# =============================================================================\n",
    "# FIT DLM\n",
    "# =============================================================================\n",
    "def fit_dlm(endog, exog, structure):\n",
    "    model = UnobservedComponents(endog=endog, exog=exog, level=structure[\"level\"],\n",
    "                                  trend=structure[\"trend\"], seasonal=structure[\"seasonal\"])\n",
    "    return model.fit(disp=False)\n",
    "\n",
    "# =============================================================================\n",
    "# REAL-TIME FORECAST\n",
    "# =============================================================================\n",
    "def realtime_recursive_forecast(series_df, ehr_df, cfg, round_ndigits=0):\n",
    "    y_lags, pos_lags, inp_lags = list(cfg[\"y_lags\"]), list(cfg[\"pos_lags\"]), list(cfg[\"inp_lags\"])\n",
    "    structure = cfg[\"structure\"]\n",
    "    series_df = series_df.copy().sort_values(\"date\").reset_index(drop=True)\n",
    "\n",
    "    origin = pd.to_datetime(series_df[\"date\"].max())\n",
    "    reference_date = origin + pd.Timedelta(days=7)\n",
    "\n",
    "    ds1, exog_cols, df_full = build_one_step_dataset_with_ehr(series_df, ehr_df, y_lags, pos_lags, inp_lags)\n",
    "    if len(ds1) < MIN_TRAIN_ROWS:\n",
    "        raise ValueError(f\"Not enough rows: {len(ds1)} < {MIN_TRAIN_ROWS}\")\n",
    "\n",
    "    res = fit_dlm(ds1[\"y\"].values.astype(float), ds1[exog_cols].values.astype(float), structure)\n",
    "\n",
    "    dates = pd.to_datetime(df_full[\"date\"])\n",
    "    origin_idx = int(df_full.index[dates == origin][0])\n",
    "\n",
    "    y_hist = df_full[\"y\"].values.astype(float)[: origin_idx + 1].copy()\n",
    "    pos_hist = df_full[\"pos\"].values.astype(float)[: origin_idx + 1].copy() if \"pos\" in df_full.columns else np.zeros(origin_idx + 1)\n",
    "    inp_hist = df_full[\"inp\"].values.astype(float)[: origin_idx + 1].copy() if \"inp\" in df_full.columns else np.zeros(origin_idx + 1)\n",
    "\n",
    "    def lag_value(hist, idx, L):\n",
    "        j = idx - L + 1\n",
    "        return float(hist[j]) if j >= 0 else float(hist[0])\n",
    "\n",
    "    out = []\n",
    "    for h in HORIZONS:\n",
    "        step_k = h + 1\n",
    "        target_date = origin + pd.Timedelta(days=7 * step_k)\n",
    "        cur_idx = origin_idx + (step_k - 1)\n",
    "\n",
    "        ex = {}\n",
    "        for L in y_lags:\n",
    "            ex[f\"y_lag{L}\"] = lag_value(y_hist, cur_idx, L)\n",
    "        for L in pos_lags:\n",
    "            ex[f\"pos_lag{L}\"] = lag_value(pos_hist, cur_idx, L)\n",
    "        for L in inp_lags:\n",
    "            ex[f\"inp_lag{L}\"] = lag_value(inp_hist, cur_idx, L)\n",
    "        ex[\"is_thanksgiving\"] = thanksgiving_flag(target_date)\n",
    "        ex[\"is_christmas\"] = christmas_flag(target_date)\n",
    "        ex[\"is_newyear\"] = newyear_flag(target_date)\n",
    "\n",
    "        x_row = np.array([[ex.get(c, 0.0) for c in exog_cols]], dtype=float)\n",
    "        fc = res.get_forecast(steps=1, exog=x_row)\n",
    "        mu = to_scalar(fc.predicted_mean)\n",
    "        try:\n",
    "            se = to_scalar(fc.se_mean)\n",
    "            if not np.isfinite(se):\n",
    "                se = 0.0\n",
    "        except:\n",
    "            se = 0.0\n",
    "\n",
    "        y_hist = np.append(y_hist, mu)\n",
    "        pos_hist = np.append(pos_hist, float(pos_hist[-1]) if len(pos_hist) else 0.0)\n",
    "        inp_hist = np.append(inp_hist, float(inp_hist[-1]) if len(inp_hist) else 0.0)\n",
    "\n",
    "        q_to_pred = {}\n",
    "        for q in SUBMISSION_QUANTILES:\n",
    "            z = norm.ppf(q)\n",
    "            xq = mu + z * se\n",
    "            yq = inv_log1p(xq) if USE_LOG_TRANSFORM else xq\n",
    "            q_to_pred[q] = round_forecast_value(yq, ndigits=round_ndigits)\n",
    "\n",
    "        out.append({\"horizon\": int(h), \"target_end_date\": target_date, \"q_to_pred\": q_to_pred})\n",
    "\n",
    "    return origin, reference_date, out\n",
    "\n",
    "# =============================================================================\n",
    "# SUBMISSION ROWS\n",
    "# =============================================================================\n",
    "def make_submission_rows(reference_date, target, horizon, target_end_date, location, q_to_value, round_ndigits=0):\n",
    "    rows = []\n",
    "    for q in SUBMISSION_QUANTILES:\n",
    "        rows.append({\n",
    "            \"reference_date\": fmt_mdY(reference_date),\n",
    "            \"target\": target,\n",
    "            \"horizon\": int(horizon),\n",
    "            \"target_end_date\": fmt_mdY(target_end_date),\n",
    "            \"location\": str(location),\n",
    "            \"output_type\": \"quantile\",\n",
    "            \"output_type_id\": _q_str(q),\n",
    "            \"value\": round_forecast_value(q_to_value[q], ndigits=round_ndigits),\n",
    "        })\n",
    "    return rows\n",
    "\n",
    "# =============================================================================\n",
    "# PLOT\n",
    "# =============================================================================\n",
    "def save_forecast_plot(loc_name, observed_df, origin_date, reference_date, forecast_df, fig_path):\n",
    "    obs = observed_df.sort_values(\"date\").copy()\n",
    "    f = forecast_df.sort_values(\"target_end_date\").copy()\n",
    "\n",
    "    fig = plt.figure(figsize=(14, 7))\n",
    "    plt.plot(pd.to_datetime(obs[\"date\"]), obs[\"y_original\"].values, linewidth=2.0, marker=\"o\", markersize=3, label=\"Observed\")\n",
    "    plt.fill_between(pd.to_datetime(f[\"target_end_date\"]), f[\"q0.025\"].values, f[\"q0.975\"].values, alpha=0.20, label=\"95% interval\")\n",
    "    plt.fill_between(pd.to_datetime(f[\"target_end_date\"]), f[\"q0.25\"].values, f[\"q0.75\"].values, alpha=0.35, label=\"50% interval\")\n",
    "    plt.plot(pd.to_datetime(f[\"target_end_date\"]), f[\"q0.50\"].values, linewidth=2.5, marker=\"s\", markersize=5, label=\"Forecast (median)\")\n",
    "    plt.axvline(pd.to_datetime(origin_date), linestyle=\"--\", linewidth=2.0, alpha=0.6, label=\"Origin\")\n",
    "    plt.title(f\"{loc_name} | {OUTCOME_MEASURE}\\nDLM recursive h=0..3 | origin={origin_date.date()} | ref={reference_date.date()}\", fontsize=12)\n",
    "    plt.xlabel(\"Date\")\n",
    "    plt.ylabel(\"Hospital admissions\")\n",
    "    plt.grid(True, alpha=0.25)\n",
    "    plt.legend(loc=\"best\")\n",
    "    plt.tight_layout()\n",
    "\n",
    "    try:\n",
    "        os.makedirs(os.path.dirname(fig_path), exist_ok=True)\n",
    "        fig.savefig(fig_path, dpi=200, bbox_inches=\"tight\")\n",
    "        print(f\"✓ Saved plot: {fig_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"WARNING: Could not save plot: {e}\")\n",
    "    plt.close()\n",
    "\n",
    "# =============================================================================\n",
    "# MAIN\n",
    "# =============================================================================\n",
    "print(\"\\n\" + \"=\"*100)\n",
    "print(\"REAL-TIME DLM RECURSIVE FORECAST — SUBMISSION FILE ONLY\")\n",
    "print(\"=\"*100)\n",
    "\n",
    "params_obj, params_path = load_optimal_params()\n",
    "ehr_sc = load_ehr_covariates_sc()\n",
    "\n",
    "loc_to_results = {}\n",
    "\n",
    "for loc in LOCATIONS:\n",
    "    loc_code, loc_name = str(loc[\"code\"]), loc[\"name\"]\n",
    "\n",
    "    print(\"\\n\" + \"=\"*100)\n",
    "    print(f\"LOCATION: {loc_name} ({loc_code})\")\n",
    "    print(\"=\"*100)\n",
    "\n",
    "    series = load_target_series_hosp(FILE_HOSP, loc_code, OUTCOME_MEASURE)\n",
    "    print(f\"Data: {len(series)} weeks | {series['date'].min().date()} to {series['date'].max().date()}\")\n",
    "\n",
    "    ehr_use = ehr_sc if (loc_code == \"45\" or APPLY_SC_EHR_TO_US) else None\n",
    "    if loc_code != \"45\" and ehr_use is not None:\n",
    "        print(\"NOTE: Using SC EHR series for US.\")\n",
    "\n",
    "    key = f\"{loc_code}_{TARGET_KEY}_dlm_recursive_with_pos_inp_prisma_musc\"\n",
    "    if key not in params_obj:\n",
    "        raise KeyError(f\"Missing params key: {key}\")\n",
    "\n",
    "    cfg0 = params_obj[key]\n",
    "    best_cfg = {\"y_lags\": list(cfg0[\"y_lags\"]), \"pos_lags\": list(cfg0[\"pos_lags\"]),\n",
    "                \"inp_lags\": list(cfg0[\"inp_lags\"]), \"structure\": cfg0[\"structure\"]}\n",
    "\n",
    "    print(f\"Config: y_lags={best_cfg['y_lags']}, pos_lags={best_cfg['pos_lags']}, inp_lags={best_cfg['inp_lags']}\")\n",
    "\n",
    "    origin_date, reference_date, fc_list = realtime_recursive_forecast(series, ehr_use, best_cfg, ROUND_NDIGITS)\n",
    "\n",
    "    print(f\"\\nOrigin: {origin_date.date()} | Reference date: {reference_date.date()}\")\n",
    "\n",
    "    plot_rows, loc_submit_rows = [], []\n",
    "    for rec in fc_list:\n",
    "        h, ted, q_to_pred = rec[\"horizon\"], rec[\"target_end_date\"], rec[\"q_to_pred\"]\n",
    "        loc_submit_rows.extend(make_submission_rows(reference_date, OUTCOME_MEASURE, h, ted, loc_code, q_to_pred, ROUND_NDIGITS))\n",
    "        plot_rows.append({\"horizon\": h, \"target_end_date\": ted, \"q0.025\": q_to_pred[0.025], \"q0.25\": q_to_pred[0.25],\n",
    "                          \"q0.50\": q_to_pred[0.50], \"q0.75\": q_to_pred[0.75], \"q0.975\": q_to_pred[0.975]})\n",
    "        print(f\"  h={h} @ {ted.date()} | median={q_to_pred[0.50]} | 50%CI=({q_to_pred[0.25]},{q_to_pred[0.75]}) | 95%CI=({q_to_pred[0.025]},{q_to_pred[0.975]})\")\n",
    "\n",
    "    # Plot\n",
    "    forecast_plot_df = pd.DataFrame(plot_rows)\n",
    "    fig_path = os.path.join(out_plot_dir, f\"{loc_code}_{TARGET_KEY}_dlm_realtime_ref_{reference_date.date()}.png\")\n",
    "    save_forecast_plot(loc_name, series, origin_date, reference_date, forecast_plot_df, fig_path)\n",
    "\n",
    "    loc_to_results[loc_code] = {\"loc_name\": loc_name, \"origin_date\": origin_date,\n",
    "                                 \"reference_date\": reference_date, \"submit_rows\": loc_submit_rows}\n",
    "\n",
    "# =============================================================================\n",
    "# SAVE SUBMISSION CSV\n",
    "# =============================================================================\n",
    "print(\"\\n\" + \"-\"*80)\n",
    "print(\"SAVING SUBMISSION FILE\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "ref_dates = [pd.to_datetime(loc_to_results[c][\"reference_date\"]) for c in loc_to_results]\n",
    "all_same_ref = all(ref_dates[0] == d for d in ref_dates) if ref_dates else False\n",
    "\n",
    "if all_same_ref and len(loc_to_results) == 2:\n",
    "    combined_rows = []\n",
    "    for loc_code in loc_to_results:\n",
    "        combined_rows.extend(loc_to_results[loc_code][\"submit_rows\"])\n",
    "\n",
    "    submit_df = pd.DataFrame(combined_rows)\n",
    "    submit_df = submit_df[[\"reference_date\", \"target\", \"horizon\", \"target_end_date\", \"location\", \"output_type\", \"output_type_id\", \"value\"]]\n",
    "    submit_df = submit_df.sort_values([\"location\", \"horizon\", \"target_end_date\", \"output_type_id\"]).reset_index(drop=True)\n",
    "\n",
    "    ref_date = ref_dates[0].date()\n",
    "    out_csv = os.path.join(out_root, f\"FluSight_submission_DLM_EHR_SC_US_ref_{ref_date}.csv\")\n",
    "    saved_path = save_with_fallback(submit_df, out_csv, \"submission CSV (SC+US)\")\n",
    "\n",
    "    print(\"\\n\" + \"=\"*100)\n",
    "    print(\"✓ SUBMISSION READY (SC+US)\")\n",
    "    print(f\"Reference date: {ref_date}\")\n",
    "    print(f\"Rows: {len(submit_df)} (2 locations × {len(HORIZONS)} horizons × 23 quantiles)\")\n",
    "    if saved_path:\n",
    "        print(f\"Saved to: {saved_path}\")\n",
    "    print(\"=\"*100)\n",
    "else:\n",
    "    for loc_code in loc_to_results:\n",
    "        submit_df = pd.DataFrame(loc_to_results[loc_code][\"submit_rows\"])\n",
    "        submit_df = submit_df[[\"reference_date\", \"target\", \"horizon\", \"target_end_date\", \"location\", \"output_type\", \"output_type_id\", \"value\"]]\n",
    "        submit_df = submit_df.sort_values([\"horizon\", \"target_end_date\", \"output_type_id\"]).reset_index(drop=True)\n",
    "        ref_date = pd.to_datetime(loc_to_results[loc_code][\"reference_date\"]).date()\n",
    "        out_csv = os.path.join(out_root, f\"FluSight_submission_DLM_EHR_{loc_code}_ref_{ref_date}.csv\")\n",
    "        save_with_fallback(submit_df, out_csv, f\"submission CSV ({loc_code})\")\n",
    "\n",
    "print(\"\\nDONE\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
